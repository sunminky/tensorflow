{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.15.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'주의할 점은, 이 옵션은 메모리의 증식만 가능하다는 것. 연산이 끝나고 메모리가 필요없는 상황이라고 해서 할당된 메모리를 반납하지 않는다. \\n\\nTensorflow 측에서는 그럴 경우 더 심한 메모리 파편화를 일으킬 수도 있다고 하니 판단은 독자의 몫.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## GPU 메모리 관리 ##\n",
    "#1번 방법\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True #메모리 수요에 따라 할당\n",
    "\n",
    "'''주의할 점은, 이 옵션은 메모리의 증식만 가능하다는 것. 연산이 끝나고 메모리가 필요없는 상황이라고 해서 할당된 메모리를 반납하지 않는다. \n",
    "\n",
    "Tensorflow 측에서는 그럴 경우 더 심한 메모리 파편화를 일으킬 수도 있다고 하니 판단은 독자의 몫.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GPU 메모리 관리 ##\n",
    "#2번 방법\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.3 #메모리 할당 비율 설정\n",
    "#config.gpu_options.allow_growth = True #메모리 수요에 따라 할당, 두개 같이 사용 가능(추천하지는 않음)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "##9.2 계산그래프를 만들어 세션에서 실행하기 ##\n",
    "x = tf.Variable(3, name=\"x\")\n",
    "y = tf.Variable(4, name=\"y\")\n",
    "f = x*x*y + y + 2\n",
    "\n",
    "sess = tf.Session(config=config)\n",
    "sess.run(x.initializer) #변수초기화\n",
    "sess.run(y.initializer) #변수초기화\n",
    "result = sess.run(f) #그래프평가(실행)\n",
    "\n",
    "print(result)\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "## with를 사용한 방법 ##\n",
    "with tf.Session(config=config) as sess:\n",
    "    #x.initializer.run() #tf.get_default_graph().run(x.initializer)와 동일\n",
    "    #y.initializer.run() #tf.get_default_graph().run(y.initializer)와 동일\n",
    "    init = tf.global_variables_initializer()\n",
    "    init.run() #모든 변수 초기화\n",
    "    result = f.eval() #tf.get_default_graph().run(f)와 동일\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession() #자기자신을 기본세션으로 지정\n",
    "init.run()\n",
    "result = f.eval()\n",
    "print(result)\n",
    "sess.close() #반드시 세션을 종료시켜야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 9.3 계산 그래프 관리 ##\n",
    "x1 = tf.Variable(1) #노드를 만들면 자동으로 기본 계산 그래프레 추가됨\n",
    "x1.graph is tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph() #독립적인 그래프 만들기\n",
    "with graph.as_default(): #임시적으로 기본그래프로 사용\n",
    "    x2 = tf.Variable(2)\n",
    "\n",
    "print(x2.graph is graph) #독립적인 그래프에 속함\n",
    "print(x2.graph is tf.get_default_graph()) #기본그래프에 속하지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() #기본그래프 초기화, 중복된 노드 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "w = tf.constant(3)\n",
    "x = w + 2\n",
    "y = x + 5\n",
    "z = x + 3\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    y_val, z_val = sess.run([y, z]) #변수는 세션이 유지되는 동안 유효함\n",
    "    print(y_val)\n",
    "    print(z_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape : (20640, 8)\n",
      "data_plus_bias shape : (20640, 9)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "housing = fetch_california_housing(\"data/\")\n",
    "m, n = housing.data.shape\n",
    "# bias 추가\n",
    "housing_data_plus_bias = np.c_[np.ones((m, 1)), housing.data]\n",
    "\n",
    "print('data shape :', housing.data.shape)\n",
    "print('data_plus_bias shape :', housing_data_plus_bias.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://render.githubusercontent.com/render/math?math=%5Chat%7B%5Cmathbf%7BW%7D%7D%20%3D%20%5Cleft%28%5Cmathbf%7BX%7D%5E%7BT%7D%20%5Cmathbf%7BX%7D%20%5Cright%29%5E%7B-1%7D%20%5Cmathbf%7BX%7D%5E%7BT%7D%20%5Cmathbf%7BY%7D&mode=display\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.68962631e+01],\n",
       "       [ 4.36777472e-01],\n",
       "       [ 9.44449380e-03],\n",
       "       [-1.07348785e-01],\n",
       "       [ 6.44962370e-01],\n",
       "       [-3.94082872e-06],\n",
       "       [-3.78797273e-03],\n",
       "       [-4.20847952e-01],\n",
       "       [-4.34020907e-01]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tf.constant(housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "Y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"Y\")\n",
    "XT = tf.transpose(X) #전치행렬로 만듬(행과 열을 바꿈)\n",
    "theta = tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(XT, X)), XT), Y) #정규방정식\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    theta_value = theta.eval()\n",
    "    \n",
    "theta_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_housing_data = scaler.fit_transform(housing.data)\n",
    "scaled_housing_data_plus_bias = np.c_[np.ones((m, 1)), scaled_housing_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE =  8.802726\n",
      "Epoch 1000 MSE =  0.53485996\n",
      "Epoch 2000 MSE =  0.5253611\n",
      "Epoch 3000 MSE =  0.52446365\n",
      "Epoch 4000 MSE =  0.5243431\n",
      "Epoch 5000 MSE =  0.5243245\n",
      "Epoch 6000 MSE =  0.52432156\n",
      "Epoch 7000 MSE =  0.524321\n",
      "Epoch 8000 MSE =  0.524321\n",
      "Epoch 9000 MSE =  0.524321\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 2.0685523 ],\n",
       "       [ 0.82964474],\n",
       "       [ 0.11875628],\n",
       "       [-0.2655744 ],\n",
       "       [ 0.3057353 ],\n",
       "       [-0.00450153],\n",
       "       [-0.03932726],\n",
       "       [-0.89982766],\n",
       "       [-0.870487  ]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 9.6 경사하강법 구현 ##\n",
    "## 9.6.1 직접 그래디언트 계산 ##\n",
    "n_epochs = 10000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "Y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"Y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0), name=\"theta\") #난수를 담은 텐서를 생성하는 노드를 그래프에 생성\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - Y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "gradients = 2/m * tf.matmul(tf.transpose(X), error)\n",
    "training_op = tf.assign(theta, theta - learning_rate * gradients) #변수에 새로운 값을 할당하는 노드 생성\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 1000 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE = \", mse.eval())\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()\n",
    "    \n",
    "best_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE =  11.879491\n",
      "Epoch 100 MSE =  0.75746316\n",
      "Epoch 200 MSE =  0.57445425\n",
      "Epoch 300 MSE =  0.55828637\n",
      "Epoch 400 MSE =  0.54878676\n",
      "Epoch 500 MSE =  0.54198045\n",
      "Epoch 600 MSE =  0.5370703\n",
      "Epoch 700 MSE =  0.5335268\n",
      "Epoch 800 MSE =  0.530969\n",
      "Epoch 900 MSE =  0.5291229\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 2.0685523 ],\n",
       "       [ 0.8021441 ],\n",
       "       [ 0.13669826],\n",
       "       [-0.17041533],\n",
       "       [ 0.20804848],\n",
       "       [ 0.00238259],\n",
       "       [-0.04025865],\n",
       "       [-0.79486364],\n",
       "       [-0.7602312 ]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 9.6.2 자동미분사용 ##\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "Y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"Y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0), name=\"theta\") #난수를 담은 텐서를 생성하는 노드를 그래프에 생성\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - Y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "#gradients = 2/m * tf.matmul(tf.transpose(X), error)\n",
    "gradients = tf.gradients(mse, [theta])[0]\n",
    "training_op = tf.assign(theta, theta - learning_rate * gradients) #변수에 새로운 값을 할당하는 노드 생성\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE = \", mse.eval())\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()\n",
    "\n",
    "best_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE =  9.297094\n",
      "Epoch 100 MSE =  0.84140295\n",
      "Epoch 200 MSE =  0.6377024\n",
      "Epoch 300 MSE =  0.6112633\n",
      "Epoch 400 MSE =  0.5950269\n",
      "Epoch 500 MSE =  0.58218277\n",
      "Epoch 600 MSE =  0.5717872\n",
      "Epoch 700 MSE =  0.56333417\n",
      "Epoch 800 MSE =  0.55644065\n",
      "Epoch 900 MSE =  0.5508048\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 2.0685523 ],\n",
       "       [ 0.97097486],\n",
       "       [ 0.15910296],\n",
       "       [-0.5066931 ],\n",
       "       [ 0.4937275 ],\n",
       "       [ 0.00834046],\n",
       "       [-0.04575343],\n",
       "       [-0.47298685],\n",
       "       [-0.45884115]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 9.6.3 옵티마이저 사용 ##\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "Y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"Y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0), name=\"theta\") #난수를 담은 텐서를 생성하는 노드를 그래프에 생성\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - Y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "#gradients = tf.gradients(mse, [theta])[0]\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "#training_op = tf.assign(theta, theta - learning_rate * gradients) #변수에 새로운 값을 할당하는 노드 생성\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE = \", mse.eval())\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()\n",
    "    \n",
    "best_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE =  1.6304214\n",
      "Epoch 10 MSE =  0.6231211\n",
      "Epoch 20 MSE =  0.53415525\n",
      "Epoch 30 MSE =  0.61872923\n",
      "Epoch 40 MSE =  0.34299383\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 2.0676532 ],\n",
       "       [ 0.81094784],\n",
       "       [ 0.13473205],\n",
       "       [-0.1928155 ],\n",
       "       [ 0.2533899 ],\n",
       "       [ 0.00396125],\n",
       "       [-0.04528483],\n",
       "       [-0.82152164],\n",
       "       [-0.78731716]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 9.7 미니배치 경사하강법 구현 ##\n",
    "n_epochs = 50\n",
    "learning_rate = 0.0001\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n+1), name=\"X\") #차원이 None이면 어떤크기도 가능하다는 뜻\n",
    "Y = tf.placeholder(tf.float32, shape=(None, 1), name=\"Y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0), name=\"theta\") #난수를 담은 텐서를 생성하는 노드를 그래프에 생성\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - Y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9) #다른 옵티마이저 사용\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "def fetch_batch(epoch, batch_index, batch_size):\n",
    "    np.random.seed(epoch * n_batches + batch_index)\n",
    "    indices = np.random.randint(m, size=batch_size)\n",
    "    X_batch = scaled_housing_data_plus_bias[indices]\n",
    "    Y_batch = housing.target.reshape(-1, 1)[indices]\n",
    "    return X_batch, Y_batch\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, Y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            sess.run(training_op, feed_dict={X : X_batch, Y : Y_batch}) #placeholder에 의존하는 식을 실행하는데 값을 주지않으면 예외발생\n",
    "        if epoch % 10 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE = \", mse.eval(feed_dict={X : X_batch, Y : Y_batch}))\n",
    "    \n",
    "    best_theta = theta.eval()\n",
    "    \n",
    "best_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE =  0.5342791\n",
      "Epoch 100 MSE =  0.59744805\n",
      "Epoch 200 MSE =  0.5149244\n",
      "Epoch 300 MSE =  0.58632696\n",
      "Epoch 400 MSE =  0.41618687\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 2.0626614 ],\n",
       "       [ 0.83224726],\n",
       "       [ 0.11040603],\n",
       "       [-0.27255896],\n",
       "       [ 0.25648004],\n",
       "       [-0.00795209],\n",
       "       [-0.07186626],\n",
       "       [-0.8915991 ],\n",
       "       [-0.8733595 ]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 9.8 모델 저장과 복원 ##\n",
    "n_epochs = 500\n",
    "learning_rate = 0.01\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n+1), name=\"X\") #차원이 None이면 어떤크기도 가능하다는 뜻\n",
    "Y = tf.placeholder(tf.float32, shape=(None, 1), name=\"Y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0), name=\"theta\") #난수를 담은 텐서를 생성하는 노드를 그래프에 생성\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - Y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "def fetch_batch(epoch, batch_index, batch_size):\n",
    "    np.random.seed(epoch * n_batches + batch_index)\n",
    "    indices = np.random.randint(m, size=batch_size)\n",
    "    X_batch = scaled_housing_data_plus_bias[indices]\n",
    "    Y_batch = housing.target.reshape(-1, 1)[indices]\n",
    "    return X_batch, Y_batch\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "#saver = tf.train.Saver() #모델저장 노드 추가\n",
    "#saver = tf.train.Saver({\"weights\" : theta}) #theta변수만 weight라는 이름으로 저장\n",
    "saver = tf.train.Saver(max_to_keep=3)\n",
    "\n",
    "#파라메터 저장#\n",
    "tf.add_to_collection('theta', theta)\n",
    "###############\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, Y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            sess.run(training_op, feed_dict={X : X_batch, Y : Y_batch}) #placeholder에 의존하는 식을 실행하는데 값을 주지않으면 예외발생\n",
    "        if epoch % 100 == 0:\n",
    "            save_path = saver.save(sess, \"data/model_save.ckpt\") #모델저장\n",
    "            print(\"Epoch\", epoch, \"MSE = \", mse.eval(feed_dict={X : X_batch, Y : Y_batch}))\n",
    "    \n",
    "    best_theta = theta.eval()\n",
    "    save_path = saver.save(sess, \"data/final_model_save.ckpt\") #모델저장\n",
    "    \n",
    "best_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from data/final_model_save.ckpt\n",
      "INFO:tensorflow:Restoring parameters from data/final_model_save.ckpt\n",
      "[[ 2.0544672 ]\n",
      " [ 0.82970107]\n",
      " [ 0.10713524]\n",
      " [-0.31074822]\n",
      " [ 0.24564512]\n",
      " [-0.00222411]\n",
      " [-0.01061386]\n",
      " [-0.891216  ]\n",
      " [-0.8752481 ]]\n",
      "Epoch 0 MSE =  0.49956453\n",
      "Epoch 10 MSE =  0.5664362\n",
      "Epoch 20 MSE =  0.5255838\n",
      "Epoch 30 MSE =  0.6023059\n",
      "Epoch 40 MSE =  0.34531453\n",
      "Epoch 50 MSE =  0.44214913\n",
      "Epoch 60 MSE =  0.5371403\n",
      "Epoch 70 MSE =  0.4436193\n",
      "Epoch 80 MSE =  0.5882714\n",
      "Epoch 90 MSE =  0.3841381\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 2.0674198 ],\n",
       "       [ 0.83073413],\n",
       "       [ 0.11769314],\n",
       "       [-0.27660346],\n",
       "       [ 0.29551786],\n",
       "       [-0.00227595],\n",
       "       [-0.0368447 ],\n",
       "       [-0.89784104],\n",
       "       [-0.8702798 ]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 9.8 모델 저장과 복원 방법1 ##\n",
    "n_epochs = 100\n",
    "learning_rate = 0.001\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    global theta\n",
    "    saver = tf.train.import_meta_graph(\"data/final_model_save.ckpt.meta\") #meta파일에 저장된 그래프 구조 복원\n",
    "    saver.restore(sess, 'data/final_model_save.ckpt')\n",
    "    theta = tf.get_collection('theta')[0] #파라메터 복원\n",
    "    #theta = tf.get_default_graph().get_tensor_by_name(\"theta:0\")\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n+1), name=\"X\") #차원이 None이면 어떤크기도 가능하다는 뜻\n",
    "Y = tf.placeholder(tf.float32, shape=(None, 1), name=\"Y\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - Y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.import_meta_graph(\"data/final_model_save.ckpt.meta\") #meta파일에 저장된 그래프 구조 복원\n",
    "\n",
    "def fetch_batch(epoch, batch_index, batch_size):\n",
    "    np.random.seed(epoch * n_batches + batch_index)\n",
    "    indices = np.random.randint(m, size=batch_size)\n",
    "    X_batch = scaled_housing_data_plus_bias[indices]\n",
    "    Y_batch = housing.target.reshape(-1, 1)[indices]\n",
    "    return X_batch, Y_batch\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    saver.restore(sess, \"data/final_model_save.ckpt\") #저장한 모델 불러오기\n",
    "    print(theta.eval())\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, Y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            sess.run(training_op, feed_dict={X : X_batch, Y : Y_batch}) #placeholder에 의존하는 식을 실행하는데 값을 주지않으면 예외발생\n",
    "        if epoch % 10 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE = \", mse.eval(feed_dict={X : X_batch, Y : Y_batch}))\n",
    "    \n",
    "    best_theta = theta.eval()\n",
    "    \n",
    "best_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from data/final_model_save.ckpt\n",
      "[[ 2.0544672 ]\n",
      " [ 0.82970107]\n",
      " [ 0.10713524]\n",
      " [-0.31074822]\n",
      " [ 0.24564512]\n",
      " [-0.00222411]\n",
      " [-0.01061386]\n",
      " [-0.891216  ]\n",
      " [-0.8752481 ]]\n",
      "Epoch 0 MSE =  0.49956453\n",
      "Epoch 10 MSE =  0.5664362\n",
      "Epoch 20 MSE =  0.5255838\n",
      "Epoch 30 MSE =  0.6023059\n",
      "Epoch 40 MSE =  0.34531453\n",
      "Epoch 50 MSE =  0.44214913\n",
      "Epoch 60 MSE =  0.5371403\n",
      "Epoch 70 MSE =  0.4436193\n",
      "Epoch 80 MSE =  0.5882714\n",
      "Epoch 90 MSE =  0.3841381\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 2.0674198 ],\n",
       "       [ 0.83073413],\n",
       "       [ 0.11769314],\n",
       "       [-0.27660346],\n",
       "       [ 0.29551786],\n",
       "       [-0.00227595],\n",
       "       [-0.0368447 ],\n",
       "       [-0.89784104],\n",
       "       [-0.8702798 ]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 모델 복원 방법2 ##\n",
    "n_epochs = 100\n",
    "learning_rate = 0.001\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "saver = tf.train.import_meta_graph(\"data/final_model_save.ckpt.meta\") #meta파일에 저장된 그래프 구조 복원, 복원된 그래프를 기본그래프에 추가 후 Saver객체 반환\n",
    "theta = tf.get_default_graph().get_tensor_by_name(\"theta:0\")\n",
    "X = tf.placeholder(tf.float32, shape=(None, n+1), name=\"X\") #차원이 None이면 어떤크기도 가능하다는 뜻\n",
    "Y = tf.placeholder(tf.float32, shape=(None, 1), name=\"Y\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - Y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "def fetch_batch(epoch, batch_index, batch_size):\n",
    "    np.random.seed(epoch * n_batches + batch_index)\n",
    "    indices = np.random.randint(m, size=batch_size)\n",
    "    X_batch = scaled_housing_data_plus_bias[indices]\n",
    "    Y_batch = housing.target.reshape(-1, 1)[indices]\n",
    "    return X_batch, Y_batch\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    saver.restore(sess, \"data/final_model_save.ckpt\") #저장한 모델 불러오기\n",
    "    print(sess.run(theta, feed_dict={X : X_batch, Y : Y_batch})) #theta의 값 출력\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, Y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            sess.run(training_op, feed_dict={X : X_batch, Y : Y_batch}) #placeholder에 의존하는 식을 실행하는데 값을 주지않으면 예외발생\n",
    "        if epoch % 10 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE = \", mse.eval(feed_dict={X : X_batch, Y : Y_batch}))\n",
    "    \n",
    "    best_theta = theta.eval()\n",
    "    \n",
    "best_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE =  7.619415\n",
      "Epoch 1000 MSE =  0.5514102\n",
      "Epoch 2000 MSE =  0.5269976\n",
      "Epoch 3000 MSE =  0.5246882\n",
      "Epoch 4000 MSE =  0.5243778\n",
      "Epoch 5000 MSE =  0.5243301\n",
      "Epoch 6000 MSE =  0.52432245\n",
      "Epoch 7000 MSE =  0.52432126\n",
      "Epoch 8000 MSE =  0.524321\n",
      "Epoch 9000 MSE =  0.524321\n"
     ]
    }
   ],
   "source": [
    "## 9.9 텐서보드로 시각화 하기 ##\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf.logs\"\n",
    "logdir = f\"{root_logdir}/run-{now}\"\n",
    "\n",
    "#### 트레이닝 ####\n",
    "tf.reset_default_graph()\n",
    "\n",
    "n_epochs = 10000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "Y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"Y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0), name=\"theta\") #난수를 담은 텐서를 생성하는 노드를 그래프에 생성\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - Y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "#gradients = tf.gradients(mse, [theta])[0]\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "#training_op = tf.assign(theta, theta - learning_rate * gradients) #변수에 새로운 값을 할당하는 노드 생성\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "mse_summary = tf.summary.scalar('MSE', mse) #MSE값을 평가하고 텐서보드가 인식하는 이진 로그 문자열에 쓰기 위한 노드(서머리)를 추가\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph()) #기록할 로그파일, 기록할 그래프 \n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 1000 == 0:\n",
    "            summary_str = mse_summary.eval()\n",
    "            step = epoch * n_batches + batch_index\n",
    "            file_writer.add_summary(summary_str, step)\n",
    "            print(\"Epoch\", epoch, \"MSE = \", mse.eval())\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()\n",
    "##########################################################################################################################################\n",
    "file_writer.close()\n",
    "\n",
    "#tensorboard --logdir [이벤트파일경로] 실행\n",
    "#http://localhost:6006/#scalars로 접속"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE =  14.490098\n",
      "Epoch 100 MSE =  1.0712796\n",
      "Epoch 200 MSE =  0.7618898\n",
      "Epoch 300 MSE =  0.69183916\n",
      "Epoch 400 MSE =  0.6450376\n",
      "Epoch 500 MSE =  0.61141586\n",
      "Epoch 600 MSE =  0.5871671\n",
      "Epoch 700 MSE =  0.56967264\n",
      "Epoch 800 MSE =  0.55705035\n",
      "Epoch 900 MSE =  0.5479429\n",
      "loss/sub\n",
      "loss/mse\n"
     ]
    }
   ],
   "source": [
    "## 9.10 이름범위 ##\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf.logs\"\n",
    "logdir = f\"{root_logdir}/run-{now}\"\n",
    "\n",
    "#### 트레이닝 ####\n",
    "tf.reset_default_graph()\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "Y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"Y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0), name=\"theta\") #난수를 담은 텐서를 생성하는 노드를 그래프에 생성\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "\n",
    "####이름범위를 만들어서 관련노드들을 그룹으로 묶음####\n",
    "with tf.name_scope(\"loss\") as scope:\n",
    "    error = y_pred - Y\n",
    "    mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "######################################################\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "mse_summary = tf.summary.scalar('MSE', mse) #MSE값을 평가하고 텐서보드가 인식하는 이진 로그 문자열에 쓰기 위한 노드(서머리)를 추가\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph()) #기록할 로그파일, 기록할 그래프 \n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            summary_str = mse_summary.eval()\n",
    "            step = epoch * n_batches + batch_index\n",
    "            file_writer.add_summary(summary_str, step)\n",
    "            print(\"Epoch\", epoch, \"MSE = \", mse.eval())\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()\n",
    "##########################################################################################################################################\n",
    "file_writer.close()\n",
    "\n",
    "print(error.op.name) #범위 내에 있는 모든 연산의 이름에는 \"loss/\"라는 접두사가 붙음\n",
    "print(mse.op.name) #범위 내에 있는 모든 연산의 이름에는 \"loss/\"라는 접두사가 붙음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 9.11 모듈화 ##\n",
    "\n",
    "def relu(X):\n",
    "    w_shape = (int(X.get_shape()[1]), 1)\n",
    "    w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")\n",
    "    b = tf.Variable(0.0, name=\"bias\")\n",
    "    z = tf.add(tf.matmul(X, w), b, name=\"z\")\n",
    "    return tf.maximum(z, 0., name=\"relu\")\n",
    "\n",
    "tf.reset_default_graph()\n",
    "n_features = 3\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "relus = [relu(X) for _ in range(5)]\n",
    "output = tf.add_n(relus, name=\"output\")\n",
    "\n",
    "file_writer = tf.summary.FileWriter(\"tf.logs/relu\", tf.get_default_graph()) #그래프저장\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 이름번위를 사용한 모듈화 ##\n",
    "\n",
    "def relu(X):\n",
    "    with tf.name_scope(\"relu\"): #더 간단하게 표시\n",
    "        w_shape = (int(X.get_shape()[1]), 1)\n",
    "        w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")\n",
    "        b = tf.Variable(0.0, name=\"bias\")\n",
    "        z = tf.add(tf.matmul(X, w), b, name=\"z\")\n",
    "    return tf.maximum(z, 0., name=\"relu\")\n",
    "\n",
    "tf.reset_default_graph()\n",
    "n_features = 3\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "relus = [relu(X) for _ in range(5)]\n",
    "output = tf.add_n(relus, name=\"output\")\n",
    "\n",
    "file_writer = tf.summary.FileWriter(\"tf.logs/relu2\", tf.get_default_graph()) #그래프저장\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 9.12 변수공유 ##\n",
    "\n",
    "def relu(X):\n",
    "    with tf.variable_scope(\"relu\", reuse=True) as scope: #변수를 재사용하기 위해 resue를 True로 설정\n",
    "        #scope.reuse_variables() #변수 재사용하기\n",
    "        threshold = tf.get_variable(\"threshold\")\n",
    "        w_shape = int(X.get_shape()[1]), 1                          # not shown\n",
    "        w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")  # not shown\n",
    "        b = tf.Variable(0.0, name=\"bias\")                           # not shown\n",
    "        z = tf.add(tf.matmul(X, w), b, name=\"z\")                    # not shown\n",
    "        return tf.maximum(z, threshold, name=\"max\")\n",
    "\n",
    "tf.reset_default_graph()\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "with tf.variable_scope(\"relu\"): #이름범위 사용, threshold변수가 함수 밖에서 선언됨\n",
    "    threshold = tf.get_variable(\"threshold\", shape=(),\n",
    "                                initializer=tf.constant_initializer(0.0)) #공유변수가 있다면 얻어옴, 없으면 생성\n",
    "relus = [relu(X) for _ in range(5)]\n",
    "output = tf.add_n(relus, name=\"output\")\n",
    "\n",
    "file_writer = tf.summary.FileWriter(\"tf.logs/relu3\", tf.get_default_graph()) #그래프저장\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(X):\n",
    "    #threshold변수가 함수 내부에서 선언됨\n",
    "    threshold = tf.get_variable(\"threshold\", shape=(),\n",
    "                                initializer=tf.constant_initializer(0.0)) #공유변수가 있다면 얻어옴, 없으면 생성\n",
    "    w_shape = int(X.get_shape()[1]), 1                          # not shown\n",
    "    w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")  # not shown\n",
    "    b = tf.Variable(0.0, name=\"bias\")                           # not shown\n",
    "    z = tf.add(tf.matmul(X, w), b, name=\"z\")                    # not shown\n",
    "    return tf.maximum(z, threshold, name=\"max\")\n",
    "\n",
    "tf.reset_default_graph()\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "relus = []\n",
    "for relu_index in range(5):\n",
    "    with tf.variable_scope(\"relu\", reuse=(relu_index >= 1)) as scope: #relu_index가 1보다 크면 재사용\n",
    "        relus.append(relu(X))\n",
    "output = tf.add_n(relus, name=\"output\")\n",
    "\n",
    "file_writer = tf.summary.FileWriter(\"tf.logs/relu4\", tf.get_default_graph()) #그래프저장\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "handsonml",
   "language": "python",
   "name": "handsonml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
