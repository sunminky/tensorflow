{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'주의할 점은, 이 옵션은 메모리의 증식만 가능하다는 것. 연산이 끝나고 메모리가 필요없는 상황이라고 해서 할당된 메모리를 반납하지 않는다. \\n\\nTensorflow 측에서는 그럴 경우 더 심한 메모리 파편화를 일으킬 수도 있다고 하니 판단은 독자의 몫.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 11.심층신경망 훈련 ##\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "## GPU 메모리 관리 ##\n",
    "#1번 방법\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True #메모리 수요에 따라 할당\n",
    "\n",
    "'''주의할 점은, 이 옵션은 메모리의 증식만 가능하다는 것. 연산이 끝나고 메모리가 필요없는 상황이라고 해서 할당된 메모리를 반납하지 않는다. \n",
    "\n",
    "Tensorflow 측에서는 그럴 경우 더 심한 메모리 파편화를 일으킬 수도 있다고 하니 판단은 독자의 몫.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "n_inputs = 28 * 28 #mnist속성크기\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\") #입력층\n",
    "Y = tf.placeholder(tf.int32, shape=(None), name=\"Y\") #출력층"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![초기화함수](img/초기화함수들.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 11.1 세이비어 초기화와 He초기화 ##\n",
    "\n",
    "with tf.name_scope(\"dnn1\"):\n",
    "    he_init = tf.variance_scaling_initializer()\n",
    "    #tf.dense함수는 기본적으로 균등분포로 세이비어 초기화 사용, he초기화 방식으로 지정하기\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\", activation=tf.nn.relu, kernel_initializer=he_init)\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, name=\"hidden2\", activation=tf.nn.relu, kernel_initializer=he_init)\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\", activation=None)\n",
    "    \n",
    "#He 초기화는 세이비어 초기화에서처럼 팬인과 팬아웃의 평균대신 팬인만 고려(variance_scaling_initializer의 기본값).\n",
    "#매개변수로 mod=\"FAN_AVG\"를 지정하여 바꿀수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>11.3.2 ELU (Exponential Linear Unit)</h3>\n",
    "<p>ELU(Exponential Linear Unit)은 2105년 B.Xu et al.의 논문에서 제안된 활성화 함수이다.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![nn](img/math.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![elu](img/elu.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elu 특\n",
    "<ul>\n",
    "    <li>z&lt;0 일때 음수값이 들어오므로 활성화 함수의 평균출력이 0에 더 가까워짐. 이는 앞서 이야기한 그래디언트 소실문제를 해결해줌.\n",
    "    하이퍼파라메터 α는 z가 큰 음수값일 때 ELU가 수렴할 값을 정의함. 보통 1로 설정하지만 변경가능</li>\n",
    "    <li>z&lt;0 이어도 그래디언트가 0이 아니므로 죽은뉴런(0만 출력하는 뉴런)을 만들지 않음</li>\n",
    "    <li>α=1일 때 이 함수는 z=0에서 급격히 변동하지 않고 z=0을 포함해 모든 구간에서 매끄러워 경사하강법의 속도를 높임</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 11.1.2 수렴하지 않는 활성화 함수 ##\n",
    "#elu함수 사용하기#\n",
    "\n",
    "with tf.name_scope(\"dnn2\"):\n",
    "    he_init = tf.variance_scaling_initializer()\n",
    "    #tf.dense함수는 기본적으로 균등분포로 세이비어 초기화 사용, he초기화 방식으로 지정하기\n",
    "    #활성화 함수를 elu로 설정\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\", activation=tf.nn.elu, kernel_initializer=he_init)\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, name=\"hidden2\", activation=tf.nn.elu, kernel_initializer=he_init)\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\", activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## leakyReLU 사용하기 ##\n",
    "def leaky_relu(z, name=None): #leakyReLU는 텐서플로우에서 제공하지 않음, 직접만들어야 함\n",
    "    return tf.maximum(0.01 * z, z, name=name)\n",
    "\n",
    "with tf.name_scope(\"dnn3\"):\n",
    "    he_init = tf.variance_scaling_initializer()\n",
    "    #tf.dense함수는 기본적으로 균등분포로 세이비어 초기화 사용, he초기화 방식으로 지정하기\n",
    "    #활성화 함수를 laekyReLU로 설정\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\", activation=leaky_relu, kernel_initializer=he_init)\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, name=\"hidden2\", activation=leaky_relu, kernel_initializer=he_init)\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\", activation=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 배치 정규화(BN, Batch Normalization)\n",
    "\n",
    "[05-1. 심층 신경망 학습](http://excelsior-cjh.tistory.com/177)에서는 활성화 함수로는 ReLU를 사용하고 He 초기화를 통해 학습 초기 단계에서의 그래디언트 소실/폭주 문제를 줄일 수 있었지만, 이러한 문제가 학습하는 동안에 또 다시 발생할 가능성이 있다.\n",
    "\n",
    "2015년 Sergety Ioffe와 Christian Szegedy는 ['Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift'](https://arxiv.org/pdf/1502.03167v3.pdf)라는 논문에서 **배치 정규화**(BN, Batch Normalization)를 제안했다.  배치 정규화는 각 층의 활성화 함수의 출력값 분포가 골고루 분포되도록 '강제'하는 방법으로, 각 층에서의 활성화 함수 출력값이 정규분포(normal distribution)를 이루도록 하는 방법이다. \n",
    "\n",
    "즉, 학습하는 동안 이전 레이어에서의 가중치 매개변수가 변함에 따라 활성화 함수 출력값의 분포가 변화하는 **내부 공변량 변화(Internal Covariate Shift) 문제를 줄이는 방법**이 바로 배치 정규화 기법이다.\n",
    "\n",
    "배치 정규화는 아래의 그림과 같이 미니배치(mini-batch)의 데이터에서 각 feature(특성)별 평균($\\mu$, mean)과 분산($\\sigma^{2}$, variance)을 구한 뒤 정규화(normalize) 해준다.\n",
    "\n",
    "![](./images/bn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "일반적으로 배치 정규화는 아래의 그림과 같이 Fully Connected(FC)나 Convolutional layer 바로 다음, 활성화 함수를 통과하기 전에 배치 정규화(BN)레이어를 삽입하여 사용한다.\n",
    "\n",
    "![](./images/bn02.png)\n",
    "\n",
    "\n",
    "\n",
    "배치 정규화는 미니배치(mini-batch)를 단위로 데이터의 분포가 평균($\\mu$, mean)이 0, 분산($\\sigma^{2}$, variance)이 1이 되도록 정규화(normalization)한다.  수식은 다음과 같다.\n",
    "\n",
    "- **Input** : 미니배치 $B = \\{ x_1, x_2, \\dots, x_m \\}$ 개의 입력 데이터, 학습 될 파라미터인 $\\gamma, \\beta$\n",
    "- **Output** : $\\{ y_i = \\text{BN}_{\\gamma, \\beta}(x_i) \\}$\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*} \\mu _{ B } & \\leftarrow  & \\frac { 1 }{ m } \\sum _{ i=1 }^{ m }{ x_{ i } } & \\text{// mini-batch mean}  \\\\ \\sigma _{ B }^{ 2 } & \\leftarrow  & \\frac { 1 }{ m } \\sum _{ i=1 }^{ m }{ \\left( x_{ i }-\\mu _{ B } \\right) ^{ 2 } } & \\text{// mini-batch variance}  \\\\ \\hat { x } _{ i } & \\leftarrow  & \\frac { x_{ i }-\\mu _{ B } }{ \\sqrt { \\sigma _{ B }^{ 2 }+\\varepsilon }} & \\text{// normalize} \\\\ y_{i} & \\leftarrow & \\gamma \\hat{x}_{i} + \\beta \\equiv \\text{BN}_{\\gamma, \\beta}(x_i) & \\text{// scale and shift} \\end{eqnarray*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\mu_B$ : 미니배치 $B$에 대한 평균\n",
    "- $\\sigma_B$ : 미니배치 $B$에 대한 표준편차\n",
    "- $m$ : 미니배치 데이터 개수\n",
    "- $\\hat{x}_i$ : 평균이 0, 분산이 1로 정규화된 입력 데이터\n",
    "- $\\gamma$ : 정규화된 데이터에 대한 스케일(scale) 조정 파라미터\n",
    "- $\\beta$ : 정규화된 데이터에 대한 이동(shift) 조정 파라미터\n",
    "- $\\varepsilon$ : 분모가 0이 되는 것을 막기 위한 작은 숫자 ($10^{-5}$)\n",
    "- $y_i$ : $\\text{BN}$ 연산의 출력 결과 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Scale($\\gamma$)과 Shift($\\beta$)를 해주는 이유\n",
    "\n",
    "위의 식에서 입력 데이터($x_i$)에 대해 정규화(normalization, $\\hat{x}_i = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_{B}^{2}+\\varepsilon}}$)를 하게 되면, $\\hat{x}_i$의 값이 대부분 0에 가까운 값이 될 것이다. 만약, 이러한 정규화된 입력 데이터 $\\hat{x}_{i}$가 시그모이드(sigmoid) 활성화 함수의 입력값으로 들어가게 되면, 비선형(nonlinearity) 함수인 sigmoid가 선형(linearity)구간에 빠지게 된다(sigmoid 함수는 0 부근에서 선형성을 띤다). 이러한 문제를 해결하기 위해서 아래의 식과 같이 정규화된 입력 데이터 $x_i$에 scaling과 shifting해주는 $\\gamma$와 $\\beta$를 적용 해준다.\n",
    "\n",
    "$$\n",
    "y_i = \\gamma \\hat{x}_i + \\beta\n",
    "$$\n",
    "\n",
    "\n",
    " $\\gamma$와 $\\beta$를 다르게 표현하면 $\\hat{x}_i$에 대한 가중치($\\gamma$) 와 편향(bias, $\\beta$)라고 볼 수 있으며,  $\\gamma$와 $\\beta$는 초기값으로 $\\gamma=1, \\beta=0$으로 시작해(즉, 초기에는 입력값 그대로 학습), 학습 과정에서 역전파(backprop)에 의해 적합한 값으로 조정된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 테스트(추론) 단계에서의 BN\n",
    "\n",
    "테스트 단계(추론 단계처럼 데이터가 하나씩 주입된다고 가정)나 추론 단계에서는 평균($\\mu_B$)과 표준편차($\\sigma_{B}$)를 계산할 미니배치가 없기 때문에 전체 Training Set의 평균과 표준편차를 사용한다. 하지만, 엄청나게 많은 전체 Training set에 대한 평균과 표준편차를 계산하기에는 무리기 때문에, 아래의 식과 같이 각 $n$개의 미니배치에 대한 평균과 표준편차를 이용해 전체 Training Set의 평균과 표준편차를 대신한다.\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\hat{\\mu} &= \\frac{1}{n} \\sum_{i=1}^{n}{\\mu_{B}^{(i)}} \\\\ \\hat{\\sigma} &= \\frac{1}{n} \\sum_{i=1}^{n}{\\sigma_{B}^{(i)}}\n",
    "\\end{align*}\n",
    "$$\n",
    "위와 같은 방법 대신, 모델 학습 단계에서 **지수 감소**(exponential decay) 이동 평균법(moving average)을 사용하여 평균과 표준편차를 계산할 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\hat{\\mu} & \\leftarrow \\alpha \\hat{\\mu} + \\left(1 - \\alpha \\right)\\mu_{B}^{(i)} & \\text{// moving mean}\\\\ \\hat{\\sigma} & \\leftarrow \\alpha \\hat{\\sigma} + \\left( 1 - \\alpha \\right) \\sigma_{B}^{(i)} & \\text{// moving stddev}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "위의 식에서 $\\alpha$는 모멘텀(momentum)값으로 일반적으로 1에 가까운 0.9, 0.99, 0.999로 설정한다.  이러한 moving mean과 moving stddev는 학습 단계에서 매 미니배치마다 업데이트 해준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 BN의 장점\n",
    "\n",
    "Batch Normalization(BN)은 논문에서 실험했던 모든 DNN의 성능이 크게 향상 시켰다. BN은 다음과 같은 장점들이 있다. \n",
    "\n",
    "- tanh나 sigmoid 같은 활성화 함수에 대해 그래디언트 소실(vanishing gradient)문제가 감소한다.\n",
    "- 가중치 초기화에 덜 민감하다. 가중치 초기값에 크게 의존하지 않기 때문에 [05-1. 심층 신경망 학습](http://excelsior-cjh.tistory.com/177)에서 알아본 가중치 초기화 기법에 대해 크게 신경 쓰지 않아도 된다.\n",
    "- 학습률(learning rate)를 크게 잡아도 gradient descent가 잘 수렴한다.\n",
    "- 오버피팅을 억제한다. BN이 마치 Regularization 역할을 하기 때문에 드롭아웃(Dropout)과 같은 규제기법에 대한 필요성이 감소한다.  하지만, BN로 인한 규제는 효과가 크지 않기 때문에 드롭아웃을 함께 사용하는 것이 좋다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 텐서플로에서 Batch Normalization 구현하기\n",
    "\n",
    "텐서플로(TensorFlow)에서는 [`tf.nn.batch_normalization()`](https://www.tensorflow.org/api_docs/python/tf/nn/batch_normalization)과 [`tf.layers.batch_normalization()`](https://www.tensorflow.org/api_docs/python/tf/layers/batch_normalization)을 통해 두 개의 Batch Normalization을 제공한다. 하지만, `tf.nn.batch_normalization`은 평균과 표준편차(학습할 때는 미니배치, 테스트할 때는 전체 데이터셋)를 직접 계산한 뒤 인자로 전달해줘야 하며, scaling과 shifting을 위한 $\\gamma, \\beta$를 만들어 줘야한다. 반면, `tf.layers.batch_normalization`은 이러한 작업들을 모두 처리해 주기 때문에 이것을 사용하는 것이 좋다. \n",
    "\n",
    "이제, `tf.layers.batch_normalization`을 이용하여 MNIST 데이터셋을 분류하는 DNN을 구현해 보도록 하자. DNN의 구성은 아래의 그림과 같이 784개의 입력층(input) → 300개 노드의 은닉층(hidden1) → 100개 노드의 은닉층(hidden2) → 10개의 출력층(output)으로 구성되어 있다.   \n",
    "\n",
    "![](./images/bn03.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![nn](img/bn03.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 11.1.3 배치정규화 ##\n",
    "tf.reset_default_graph()\n",
    "\n",
    "n_inputs = 28 * 28 #mnist속성크기\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\") #입력층\n",
    "\n",
    "training = tf.placeholder_with_default(False, shape=(), name=\"training\")\n",
    "\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\")\n",
    "bn1 = tf.layers.batch_normalization(hidden1, training=training, momentum=0.9) #입력데이터 배치정규화\n",
    "bn1_act = tf.nn.relu(bn1)\n",
    "hidden2 = tf.layers.dense(bn1_act, n_hidden1, name=\"hidden2\")\n",
    "bn2 = tf.layers.batch_normalization(hidden2, training=training, momentum=0.9) #입력데이터 배치정규화\n",
    "bn2_act = tf.nn.relu(bn2)\n",
    "logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name=\"outputs\")\n",
    "logits = tf.layers.batch_normalization(logits_before_bn, training=training, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "handsonml",
   "language": "python",
   "name": "handsonml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
